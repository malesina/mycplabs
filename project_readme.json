{"markdown":"# COVID-19 Tracking with IBM DataStage\n\n\n## Description\n\nIBM DataStage is a data integration tool for designing, developing, and running jobs that move and transform data.\n\nDataStage is one of the data integration components of Watson Studio. The DataStage service is fully integrated into Cloud Pak for Data as a Service as part of the data fabric. It provides a graphical framework for developing the jobs that move data from source systems to target systems. The transformed data can be delivered to data warehouses, data marts, and operational data stores, real-time web services and messaging systems, and other enterprise applications. DataStage supports extract, transform, and load (ETL) and extract, load, and transform (ELT) patterns. DataStage uses parallel processing and enterprise connectivity to provide a truly scalable platform.\n\nThis project uses a DataStage flow to join two New York Times COVID-19 public datasets together. The data sets are filtered on case counts in the city of Providence, Rhode Island in the United States of America. The datasets pull data from The New York Times, based on reports from state and local health agencies.\n\nAfter running this project, you'll be familiar with how to perform filter and join operations within DataStage by using live HTTP connections to the public dataset.\n\n\n**Tip**: Download the PDF of these instructions from the Data assets section on the Assets page so you can keep these instructions open while you work.\n\n## Data assets\nThis project contains the following data assets:\n- Two data connections\n- One DataStage flow\n- One DataStage job runtime asset that is tied to the DataStage flow\n\n## Before you begin\nTo complete this project, you must have an instance of DataStage provisioned already. To verify if you already have one, and add one if needed, complete the following steps:\n1. From the navigation menu, click **Services** > **Service instances**. The Service instances page appears. \n1. If no instances of DataStage are visible, add one by clicking **Add service**, then selecting **DataStage**.\n\n## Instructions\nFollow these instructions to learn how to perform filter and join operations and tour other features of DataStage.\n\n### Use data connections\n1. On the **Data Assets** tab, select the pre-configured **COVID Tracking: Colleges** connection.\n1. Click **Test connection** to test the connection.\n\n\nYou can create connection assets that connect to a variety of data sources and targets. Check out the [sources and targets](https://dataplatform.cloud.ibm.com/docs/content/wsj/manage-data/datastage-connectors.html) that are supported by DataStage. A connection asset contains the information necessary to create a connection to a data source or target. \n\nYou can use connection assets from within the connectors on the DataStage flow canvas. The same connection can also be used in other products, such as IBM Watson® Studio or Watson Knowledge Catalog.\n\n### Work with a DataStage flow\nNavigate back to the **Assets** tab in your Project. You will now work with the new DataStage flow canvas. Select the existing flow, **COVID Tracking: Colleges in Providence**, to open the canvas.\n\n\nExplore the palette of connectors and stages that can be used to build DataStage flows. Interact with the canvas by dragging connectors and stages around, detaching links and double-clicking connectors or stages to see their configuration properties.\n\n\nThis sample DataStage flow extracts data from two HTTP sources, filters the results of one of the sources, joins the two datasets together on a common key, and writes the resulting output dataset to the job log by using a Peek stage.\n\n**View the connections**\n1. When you’ve become familiar with the canvas, open the **Colleges** HTTP connector by double-clicking the  connector on the canvas.\n\n   In the **Properties** section, notice the **Select connection** drop-down menu that you can use to select from existing HTTP connections that were defined within the project. Selecting the connection populates all connection property configurations within the connector.\n\n1. Select the **Output** tab within the connector properties, then open the **Columns** section, then click **Edit**. The **Edit Output Columns** window appears.\n\n   You can click individual columns to select them for editing. You can select multiple columns at the same time to bulk reorder or delete columns.\n1. Click **Apply and return** to go back to the HTTP connector properties.\n1. Click **Save** to close the connector properties window.\n\n**Exploring the filter condition**\n1. Double-click the Filter stage to open its properties.\n1. Open the **Properties** section and note the filter condition, which is located under **Predicates**. Filter conditions support standard SQL expressions. This expression returns all records pertinent to the city of Providence.\n\n**Exploring the join condition**\n1.  Double-click the Join stage to open its properties.\n1. Open the **Properties** section and note the join key. This the key that will be used to perform a left outer-join of the two input streams, the **College** and **FIPS** datasets.\n1. Select the **Output** tab in the Join stage properties, then open the **Columns** section to view the column-mapping details.\n1. Click **Edit** to view the **Edit columns** window. This view displays the joined dataset and how the inputs into the join stage will make up the target metadata. From here you can select and map columns from the inputs to the target dataset.\n\n### Running a job\n1. Click **Run** on the canvas toolbar to save, compile, and run the DataStage flow.\n1. Click **Logs** to open the log panel.\n   When the job completes, you see a banner that indicates that the job is finished and successful. The log panel has a type-ahead search and filtering capability that refreshes as new entries come into the log.\n1. Return to the project dashboard by clicking the project name in the breadcrumb view on the canvas.\n\n**Interacting with jobs and viewing logs**\n\nA job is a platform runtime asset that is related to and associated with a flow. Multiple jobs can be associated with the same flow. Jobs can be scheduled or run as needed. \n\nJobs are automatically created for you when you edit or work with a DataStage flow in the canvas. When you click **Run** on the canvas toolbar, a job is created and invoked. Jobs maintain their past invocations and logs, which you can view on the jobs dashboard.\n\nA flow has a one-to-many relationship with the job asset type, meaning any number of jobs can be associated with a single flow. \n\n**Creating a job**\n1. Go to the **Assets** tab of the project, then go to the **COVID Tracking - Colleges in Providence** flow. Click the overflow menu, then click **Create job**.\n1. Follow the steps that the job creation wizard provides. The job creation wizard takes you through the available options for a job, including specifying a scheduled run.\n\n**Checking results of a job**\n\nWithin the project dashboard, select the **Jobs** tab to display all jobs across your project. Select a job from the list to view past executions. \n\n**Tip**: For further instruction coverage, check out this [COVID19 Tracking Blog](https://developer.ibm.com/tutorials/getting-started-using-the-new-ibm-datastage-saas-beta-service/).\n\n## Data connections\n\n- `COVID Tracking: Colleges`: Connection configuration to the latest US colleges data set in the repo.\n- `COVID Tracking: FIPS`: Connection configuration to the US counties data set in the repo.\n\n## DataStage flow\n\n- `COVID Tracking: Colleges in Providence`: The flow canvas to extract and customize the data sets listed.\n\n## Resources\n\n- [New York Times COVID-19 Public Datasets](https://github.com/nytimes/covid-19-data)\n- [The New York Times](https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html.)\n- [DataStage usage documentation](https://dataplatform.cloud.ibm.com/docs/content/dstage/dsnav/topics/datastage.html?audience=wdp)\n\n<hr>\n\n### This project comprises of IBM service demonstration generated by approved third-party data and does not contain IBM proprietary information."}